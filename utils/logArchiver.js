const fs = require('fs').promises;const path = require('path');const { createGzip } = require('zlib');const { promisify } = require('util');const { pipeline } = require('stream');const { createReadStream, createWriteStream } = require('fs');const { v4: uuidv4 } = require('uuid');const pipelineAsync = promisify(pipeline);class LogArchiver {  constructor(client) {    this.client = client;    this.enabled = true;    this.archiveDir = './archives';    this.tempDir = './temp';    this.retentionPeriods = {      system: 90,      security: 365,      command: 30,      api: 14,      error: 90,      userActivity: 60,      performance: 30,      automation: 30,      default: 30    };    this.compressionLevel = 9;    this.archiveSchedule = 24 * 60 * 60 * 1000;    this.cleanupSchedule = 7 * 24 * 60 * 60 * 1000;    this.lastArchiveDate = null;    this.lastCleanupDate = null;    this.initialize();  }  async initialize() {    await this.ensureDirectoriesExist();    setInterval(() => this.scheduleArchiving(), 60 * 60 * 1000);    setInterval(() => this.scheduleCleanup(), 24 * 60 * 60 * 1000);    console.log('Log archiving system initialized');    return true;  }  async ensureDirectoriesExist() {    await fs.mkdir(this.archiveDir, { recursive: true });    await fs.mkdir(this.tempDir, { recursive: true });    const logTypes = Object.keys(this.retentionPeriods);    for (const logType of logTypes) {      const typePath = path.join(this.archiveDir, logType);      await fs.mkdir(typePath, { recursive: true });    }  }  async scheduleArchiving() {    const now = new Date();    const today = new Date(now.getFullYear(), now.getMonth(), now.getDate());    if (!this.lastArchiveDate || ((now - this.lastArchiveDate) >= this.archiveSchedule)) {      try {        console.log('Starting scheduled log archiving...');        await this.archiveLogs();        this.lastArchiveDate = now;        if (this.client && this.client.customLogs) {          this.client.customLogs.log('system', {            severity: 'INFO',            content: 'Scheduled log archiving completed',            details: {              timestamp: now.toISOString(),              nextScheduled: new Date(now.getTime() + this.archiveSchedule).toISOString()            }          });        }      } catch (error) {        console.error('Error during scheduled archiving:', error);        if (this.client && this.client.customLogs) {          this.client.customLogs.logError(error, 'Log archiving', null, null);        }      }    }  }  async scheduleCleanup() {    const now = new Date();    if (!this.lastCleanupDate || ((now - this.lastCleanupDate) >= this.cleanupSchedule)) {      try {        console.log('Starting scheduled archive cleanup...');        await this.cleanupOldArchives();        this.lastCleanupDate = now;        if (this.client && this.client.customLogs) {          this.client.customLogs.log('system', {            severity: 'INFO',            content: 'Scheduled archive cleanup completed',            details: {              timestamp: now.toISOString(),              nextScheduled: new Date(now.getTime() + this.cleanupSchedule).toISOString()            }          });        }      } catch (error) {        console.error('Error during scheduled cleanup:', error);        if (this.client && this.client.customLogs) {          this.client.customLogs.logError(error, 'Archive cleanup', null, null);        }      }    }  }  async archiveLogs() {    if (!this.enabled) return;    const startTime = Date.now();    if (!this.client.customLogs) {      throw new Error('Custom logs system not initialized');    }    const inMemoryStorage = this.client.customLogs.exportStorageToObject();    if (!inMemoryStorage || Object.keys(inMemoryStorage).length === 0) {      console.log('No logs to archive');      return;    }    const today = new Date();    const dateStr = today.toISOString().split('T')[0];    let totalLogs = 0;    let fileCount = 0;    for (const [storageKey, items] of Object.entries(inMemoryStorage)) {      if (!items || items.length === 0) continue;      const logType = storageKey.replace('Storage', '');      const archiveFileName = `${logType}_${dateStr}_${uuidv4().substring(0, 8)}.json.gz`;      const tempFilePath = path.join(this.tempDir, `${archiveFileName}.tmp`);      const archiveFilePath = path.join(this.archiveDir, logType, archiveFileName);      try {        await fs.writeFile(tempFilePath, JSON.stringify(items, null, 2));        await this.compressFile(tempFilePath, archiveFilePath);        totalLogs += items.length;        fileCount++;        await fs.unlink(tempFilePath).catch(() => {});      } catch (error) {        console.error(`Error archiving ${logType} logs:`, error);        if (this.client.customLogs) {          this.client.customLogs.logError(error, 'Log archiving', null, null);        }      }    }    const processingTime = Date.now() - startTime;    console.log(`Archived ${totalLogs} logs in ${fileCount} files (${processingTime}ms)`);    if (this.client.customLogs) {      this.client.customLogs.log('system', {        severity: 'INFO',        content: `Log archiving completed`,        details: {          totalLogs,          fileCount,          processingTime,          date: dateStr        }      });    }    return {      totalLogs,      fileCount,      processingTime    };  }  async compressFile(inputPath, outputPath) {    const gzip = createGzip({ level: this.compressionLevel });    const source = createReadStream(inputPath);    const destination = createWriteStream(outputPath);    await pipelineAsync(source, gzip, destination);  }  async extractArchive(archivePath, outputPath) {    const { createGunzip } = require('zlib');    const gunzip = createGunzip();    const source = createReadStream(archivePath);    const destination = createWriteStream(outputPath);    await pipelineAsync(source, gunzip, destination);    const fileContent = await fs.readFile(outputPath, 'utf8');    return JSON.parse(fileContent);  }  async cleanupOldArchives() {    if (!this.enabled) return;    const now = Date.now();    let deletedFiles = 0;    let freedSpace = 0;    for (const [logType, retentionDays] of Object.entries(this.retentionPeriods)) {      const archivePath = path.join(this.archiveDir, logType);      try {        const files = await fs.readdir(archivePath);        for (const file of files) {          const filePath = path.join(archivePath, file);          const stats = await fs.stat(filePath);          const fileAge = now - stats.mtime.getTime();          const maxAge = retentionDays * 24 * 60 * 60 * 1000;          if (fileAge > maxAge) {            freedSpace += stats.size;            await fs.unlink(filePath);            deletedFiles++;          }        }      } catch (error) {        console.error(`Error cleaning up ${logType} archives:`, error);        if (this.client.customLogs) {          this.client.customLogs.logError(error, 'Archive cleanup', null, null);        }      }    }    console.log(`Deleted ${deletedFiles} old archive files, freed ${(freedSpace / 1024 / 1024).toFixed(2)} MB`);    if (this.client.customLogs) {      this.client.customLogs.log('system', {        severity: 'INFO',        content: `Archive cleanup completed`,        details: {          deletedFiles,          freedSpaceKB: Math.round(freedSpace / 1024),          timestamp: new Date().toISOString()        }      });    }    return {      deletedFiles,      freedSpace    };  }  async searchArchivedLogs(options = {}) {    const { logType, startDate, endDate, searchTerm, limit = 100 } = options;    if (!logType) {      throw new Error('Log type is required for archive search');    }    const archivePath = path.join(this.archiveDir, logType);    let results = [];    try {      const files = await fs.readdir(archivePath);      const filteredFiles = files.filter(file => {        if (!file.endsWith('.json.gz')) return false;        if (startDate || endDate) {          const dateMatch = file.match(/(\d{4}-\d{2}-\d{2})/);          if (dateMatch) {            const fileDate = new Date(dateMatch[1]);            if (startDate && fileDate < new Date(startDate)) return false;            if (endDate && fileDate > new Date(endDate)) return false;          }        }        return true;      });      const tempOutputPath = path.join(this.tempDir, `search_${Date.now()}.json`);      for (const file of filteredFiles) {        if (results.length >= limit) break;        const filePath = path.join(archivePath, file);        try {          const logs = await this.extractArchive(filePath, tempOutputPath);          const matchedLogs = logs.filter(log => {            if (searchTerm) {              const content = log.content || '';              const stringifiedDetails = JSON.stringify(log.details || {});              if (!content.includes(searchTerm) && !stringifiedDetails.includes(searchTerm)) {                return false;              }            }            return true;          });          results = [...results, ...matchedLogs];          if (results.length > limit) {            results = results.slice(0, limit);          }        } catch (error) {          console.error(`Error searching archive ${file}:`, error);        }      }      await fs.unlink(tempOutputPath).catch(() => {});      return results;    } catch (error) {      console.error(`Error searching archived logs:`, error);      if (this.client.customLogs) {        this.client.customLogs.logError(error, 'Archive search', null, null);      }      return [];    }  }}module.exports = LogArchiver; 